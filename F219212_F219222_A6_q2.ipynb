{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nWfH6vh1bjg_"
      },
      "outputs": [],
      "source": [
        "!pip install git+https://github.com/openai/whisper.git\n",
        "!pip install transformers\n",
        "!pip install torchvision\n",
        "!pip install gradio\n",
        "!pip install pyttsx3\n",
        "!pip install sentencepiece\n",
        "!pip install torchaudio\n",
        "!pip install git+https://github.com/huggingface/transformers.git\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import whisper\n",
        "import tempfile\n",
        "import torchaudio\n",
        "\n",
        "model_asr = whisper.load_model(\"small\")\n",
        "\n",
        "def transcribe(audio):\n",
        "    with tempfile.NamedTemporaryFile(suffix=\".wav\", delete=False) as tmp:\n",
        "        tmp.write(audio.read())\n",
        "        result = model_asr.transcribe(tmp.name)\n",
        "    return result[\"text\"]\n"
      ],
      "metadata": {
        "id": "PJDA3WLabzko"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Blip2Processor, Blip2ForConditionalGeneration\n",
        "from PIL import Image\n",
        "import torch\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "processor = Blip2Processor.from_pretrained(\"Salesforce/blip2-flan-t5-small\")\n",
        "model_vqa = Blip2ForConditionalGeneration.from_pretrained(\"Salesforce/blip2-flan-t5-small\", torch_dtype=torch.float16 if device == \"cuda\" else torch.float32)\n",
        "model_vqa.to(device)\n",
        "\n",
        "def generate_answer(image, question):\n",
        "    inputs = processor(images=image, text=question, return_tensors=\"pt\").to(device, torch.float16 if device == \"cuda\" else torch.float32)\n",
        "    output = model_vqa.generate(**inputs, max_new_tokens=50)\n",
        "    return processor.decode(output[0], skip_special_tokens=True)\n"
      ],
      "metadata": {
        "id": "FdelpjJEb0_Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pyttsx3\n",
        "import tempfile\n",
        "import os\n",
        "\n",
        "def speak_text(text):\n",
        "    engine = pyttsx3.init()\n",
        "    _, path = tempfile.mkstemp(suffix=\".mp3\")\n",
        "    engine.save_to_file(text, path)\n",
        "    engine.runAndWait()\n",
        "    return path\n"
      ],
      "metadata": {
        "id": "Z3tODXT3b2cJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "from asr import transcribe\n",
        "from qa import generate_answer\n",
        "from tts import speak_text\n",
        "from PIL import Image\n",
        "\n",
        "def process_pipeline(audio, image):\n",
        "    question = transcribe(audio)\n",
        "    answer = generate_answer(image, question)\n",
        "    audio_path = speak_text(answer)\n",
        "    return question, answer, audio_path\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "    with gr.Row():\n",
        "        audio_input = gr.Audio(source=\"microphone\", type=\"file\", label=\"Speak Your Question\")\n",
        "        image_input = gr.Image(type=\"pil\", label=\"Upload an Image\")\n",
        "    with gr.Row():\n",
        "        submit_btn = gr.Button(\"Ask the Image\")\n",
        "    with gr.Row():\n",
        "        question_output = gr.Textbox(label=\"Transcribed Question\")\n",
        "        answer_output = gr.Textbox(label=\"Generated Answer\")\n",
        "        audio_output = gr.Audio(label=\"Answer (Spoken)\")\n",
        "    submit_btn.click(fn=process_pipeline, inputs=[audio_input, image_input], outputs=[question_output, answer_output, audio_output])\n",
        "\n",
        "demo.launch()\n"
      ],
      "metadata": {
        "id": "3CPAsCm6b37q"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}